{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MIND: In-Network Memory Management for Disaggregated Data Centers \u00b6 Overview \u00b6 MIND is an in-network memory management system for compute-memory disaggregation. What is resource disaggregation and its benefits? \u00b6 Resource disaggregation physically seprates compute and memory into network-attached resource blades, which can provide higher resource utilization, better support for hardware heterogeneity, resource elasticity, and failure handling, compared to traditional data center architectures.","title":"About"},{"location":"#mind-in-network-memory-management-for-disaggregated-data-centers","text":"","title":"MIND: In-Network Memory Management for Disaggregated Data Centers"},{"location":"#overview","text":"MIND is an in-network memory management system for compute-memory disaggregation.","title":"Overview"},{"location":"#what-is-resource-disaggregation-and-its-benefits","text":"Resource disaggregation physically seprates compute and memory into network-attached resource blades, which can provide higher resource utilization, better support for hardware heterogeneity, resource elasticity, and failure handling, compared to traditional data center architectures.","title":"What is resource disaggregation and its benefits?"},{"location":"api/","text":"Confluo API \u00b6 C++ API Client APIs C++ Client API Python Client API Java Client API","title":"Confluo API"},{"location":"api/#confluo-api","text":"C++ API Client APIs C++ Client API Python Client API Java Client API","title":"Confluo API"},{"location":"applications/","text":"Case Studies \u00b6 We have evaluated Confluo for a wide range of applications. Here, we describe the design and implementation for three such applications: A network monitoring and diagnosis tool A time-series database A pub-sub system We also evaluate each of these applications against state-of-the art approaches in their respective domains.","title":"Case Studies"},{"location":"applications/#case-studies","text":"We have evaluated Confluo for a wide range of applications. Here, we describe the design and implementation for three such applications: A network monitoring and diagnosis tool A time-series database A pub-sub system We also evaluate each of these applications against state-of-the art approaches in their respective domains.","title":"Case Studies"},{"location":"client_api/","text":"Client API \u00b6 C++ Client API Python Client API Java Client API","title":"Client API"},{"location":"client_api/#client-api","text":"C++ Client API Python Client API Java Client API","title":"Client API"},{"location":"contact/","text":"People \u00b6 Seung-seob Lee Postdoctoral Associate at Yale seung-seob.lee AT yale.edu Yanpeng Yu Graduate Student at Yale yanpeng.yu AT yale.edu Yupeng Tang Graduate Student at Yale yupeng.tang AT yale.edu Anurag Khandelwal Assistant Professor at Yale anurag.khandelwal AT yale.edu Lin Zhong Professor at Yale lin.zhong AT yale.edu Abhishek Bhattacharjee Associate Professor at Yale abhishek.bhattacharjee AT yale.edu","title":"MIND Team"},{"location":"contact/#people","text":"Seung-seob Lee Postdoctoral Associate at Yale seung-seob.lee AT yale.edu Yanpeng Yu Graduate Student at Yale yanpeng.yu AT yale.edu Yupeng Tang Graduate Student at Yale yupeng.tang AT yale.edu Anurag Khandelwal Assistant Professor at Yale anurag.khandelwal AT yale.edu Lin Zhong Professor at Yale lin.zhong AT yale.edu Abhishek Bhattacharjee Associate Professor at Yale abhishek.bhattacharjee AT yale.edu","title":"People"},{"location":"contributing/","text":"Contributing \u00b6 Please contribute by creating pull-requests and filing issues on our GitHub repository. When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Please note we have a code of conduct , please follow it in all your interactions with the project. Pull Request Process \u00b6 Ensure any install or build dependencies are removed before the end of the layer when doing a build. Update the documentation under docs/ with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and code examples. Increase the version numbers in the project's root CMakeLists.txt to the new version that this Pull Request would represent. The versioning scheme we use is SemVer . You may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.","title":"Contributing"},{"location":"contributing/#contributing","text":"Please contribute by creating pull-requests and filing issues on our GitHub repository. When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change. Please note we have a code of conduct , please follow it in all your interactions with the project.","title":"Contributing"},{"location":"contributing/#pull-request-process","text":"Ensure any install or build dependencies are removed before the end of the layer when doing a build. Update the documentation under docs/ with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and code examples. Increase the version numbers in the project's root CMakeLists.txt to the new version that this Pull Request would represent. The versioning scheme we use is SemVer . You may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.","title":"Pull Request Process"},{"location":"cpp_api/","text":"C++ API Documentation \u00b6 This page will be replaced by API documentation generated by Doxygen.","title":"C++ API Documentation"},{"location":"cpp_api/#c-api-documentation","text":"This page will be replaced by API documentation generated by Doxygen.","title":"C++ API Documentation"},{"location":"cpp_client_api/","text":"C++ Client API Documentation \u00b6 This page will be replaced by API documentation generated by Doxygen.","title":"C++ Client API Documentation"},{"location":"cpp_client_api/#c-client-api-documentation","text":"This page will be replaced by API documentation generated by Doxygen.","title":"C++ Client API Documentation"},{"location":"data_archival/","text":"Archiving Data \u00b6 Confluo can limit the amount of data resident in memory by archiving old data. It does this by periodically archiving records in the data log, as well as all corresponding filter and index data, up to a data log offset. Archived data is accessed in the same way as data in memory since it is memory-mapped after it is persisted. This data is written to disk either in its existing format, or in a compressed representation, depending on configuration parameters. By default, the data log is compressed using LZ4. Filter and index data are delta compressed. During reads, all decompression is done under the hood. The archiver does not affect any readers that are concurrently accessing data since pointers to Confluo's data structures have associated reference counts that prevent premature deallocation. Readers obtain these pointers atomically. Usage \u00b6 Periodic Archival \u00b6 To initialize a multilog with periodic archiving capabilities: auto archival_mode = confluo::archival::periodic_archival_mode::ON; store.create_atomic_multilog(\"my_log\", schema, storage::IN_MEMORY, archival_mode); Alternatively, for an existing multilog, we can toggle periodic archival: mlog->set_periodic_archival(confluo::archival::periodic_archival_mode::ON); mlog->set_periodic_archival(confluo::archival::periodic_archival_mode::OFF); By default, if archival is on, the archiver will run periodically every 5 minutes. This can be changed through configuration parameters. The maximum amount of data log data in memory can also be configured. archival_periodicity_ms archival_in_memory_datalog_window_bytes *TODO better name for above param* Forced Archival \u00b6 Regardless of whether archival is turned on for a particular multilog, the user can force archival up to a data log offset by calling: mlog->archive(); // Archive up to the read tail mlog->archive(offset); // Archive up to any offset before the read tail Allocator-triggered Archival \u00b6 In cases where the periodic archiver cannot keep up with write pressure and the maximum memory of the system is reached, the allocator will block until Confluo makes more memory available, which is done by archiving the multilogs aggressively. All multilogs are archived in their entirety to make space for newer data. This can be configured by changing: max_memory","title":"Archiving Data"},{"location":"data_archival/#archiving-data","text":"Confluo can limit the amount of data resident in memory by archiving old data. It does this by periodically archiving records in the data log, as well as all corresponding filter and index data, up to a data log offset. Archived data is accessed in the same way as data in memory since it is memory-mapped after it is persisted. This data is written to disk either in its existing format, or in a compressed representation, depending on configuration parameters. By default, the data log is compressed using LZ4. Filter and index data are delta compressed. During reads, all decompression is done under the hood. The archiver does not affect any readers that are concurrently accessing data since pointers to Confluo's data structures have associated reference counts that prevent premature deallocation. Readers obtain these pointers atomically.","title":"Archiving Data"},{"location":"data_archival/#usage","text":"","title":"Usage"},{"location":"data_archival/#periodic-archival","text":"To initialize a multilog with periodic archiving capabilities: auto archival_mode = confluo::archival::periodic_archival_mode::ON; store.create_atomic_multilog(\"my_log\", schema, storage::IN_MEMORY, archival_mode); Alternatively, for an existing multilog, we can toggle periodic archival: mlog->set_periodic_archival(confluo::archival::periodic_archival_mode::ON); mlog->set_periodic_archival(confluo::archival::periodic_archival_mode::OFF); By default, if archival is on, the archiver will run periodically every 5 minutes. This can be changed through configuration parameters. The maximum amount of data log data in memory can also be configured. archival_periodicity_ms archival_in_memory_datalog_window_bytes *TODO better name for above param*","title":"Periodic Archival"},{"location":"data_archival/#forced-archival","text":"Regardless of whether archival is turned on for a particular multilog, the user can force archival up to a data log offset by calling: mlog->archive(); // Archive up to the read tail mlog->archive(offset); // Archive up to any offset before the read tail","title":"Forced Archival"},{"location":"data_archival/#allocator-triggered-archival","text":"In cases where the periodic archiver cannot keep up with write pressure and the maximum memory of the system is reached, the allocator will block until Confluo makes more memory available, which is done by archiving the multilogs aggressively. All multilogs are archived in their entirety to make space for newer data. This can be configured by changing: max_memory","title":"Allocator-triggered Archival"},{"location":"install/","text":"Installation \u00b6 Before you can install Confluo, make sure you have the following prerequisites: MacOS X or Unix-based OS; Windows is not yet supported. C++ compiler that supports C++11 standard (e.g., GCC 5.3 or later) CMake 3.2 or later Boost 1.58 or later For python client, you will additionally require: Python 2.7 or later Python Packages: setuptools, six 1.7.2 or later For java client, you will additionally require: Java 1.7 or later ant 1.6.2 or later Download \u00b6 You can obtain the latest version of Confluo by cloning the GitHub repository: git clone https://github.com/ucbrise/confluo.git Configure \u00b6 To configure the build, Confluo uses CMake as its build system. Confluo only supports out of source builds; the simplest way to configure the build would be as follows: cd confluo mkdir -p build cd build cmake .. It is possible to configure the build specifying certain options based on requirements; the supported options are: BUILD_TESTS : Builds all tests (ON by default) BUILD_RPC : Builds the rpc daemon and client libraries (ON by default) BUILD_EXAMPLES : Builds Confluo examples (ON by default) BUILD_DOC : Builds Confluo documentation (OFF by default) WITH_PY_CLIENT : Builds Confluo python rpc client (ON by default) WITH_JAVA_CLIENT : Builds Confluo java rpc client (ON by default) In order to explicitly enable or disable any of these options, set the value of the corresponding variable to ON or OFF as follows: cmake -DBUILD_TESTS=OFF Finally, you can configure the install location for Confluo by modifying the CMAKE_INSTALL_PREFIX variable (which is set to /usr/local by default): cmake -DCMAKE_INSTALL_PREFIX=/path/to/installation Install \u00b6 Once the build is configured, you can proceed to compile, test and install Confluo. To build, use: make or make -j{NUM_CORES} to speed up the build on multi-core systems. To run the various unit tests, run: make test and finally, to install, use: make install","title":"Installation"},{"location":"install/#installation","text":"Before you can install Confluo, make sure you have the following prerequisites: MacOS X or Unix-based OS; Windows is not yet supported. C++ compiler that supports C++11 standard (e.g., GCC 5.3 or later) CMake 3.2 or later Boost 1.58 or later For python client, you will additionally require: Python 2.7 or later Python Packages: setuptools, six 1.7.2 or later For java client, you will additionally require: Java 1.7 or later ant 1.6.2 or later","title":"Installation"},{"location":"install/#download","text":"You can obtain the latest version of Confluo by cloning the GitHub repository: git clone https://github.com/ucbrise/confluo.git","title":"Download"},{"location":"install/#configure","text":"To configure the build, Confluo uses CMake as its build system. Confluo only supports out of source builds; the simplest way to configure the build would be as follows: cd confluo mkdir -p build cd build cmake .. It is possible to configure the build specifying certain options based on requirements; the supported options are: BUILD_TESTS : Builds all tests (ON by default) BUILD_RPC : Builds the rpc daemon and client libraries (ON by default) BUILD_EXAMPLES : Builds Confluo examples (ON by default) BUILD_DOC : Builds Confluo documentation (OFF by default) WITH_PY_CLIENT : Builds Confluo python rpc client (ON by default) WITH_JAVA_CLIENT : Builds Confluo java rpc client (ON by default) In order to explicitly enable or disable any of these options, set the value of the corresponding variable to ON or OFF as follows: cmake -DBUILD_TESTS=OFF Finally, you can configure the install location for Confluo by modifying the CMAKE_INSTALL_PREFIX variable (which is set to /usr/local by default): cmake -DCMAKE_INSTALL_PREFIX=/path/to/installation","title":"Configure"},{"location":"install/#install","text":"Once the build is configured, you can proceed to compile, test and install Confluo. To build, use: make or make -j{NUM_CORES} to speed up the build on multi-core systems. To run the various unit tests, run: make test and finally, to install, use: make install","title":"Install"},{"location":"java_client_api/","text":"Java Client API Documentation \u00b6 This page will be replaced by API documentation generated by javadocs.","title":"Java Client API Documentation"},{"location":"java_client_api/#java-client-api-documentation","text":"This page will be replaced by API documentation generated by javadocs.","title":"Java Client API Documentation"},{"location":"loading_data/","text":"Data Storage \u00b6 Confluo operates on data streams . Each stream comprises of records, each of which follows a pre-defined schema over a collection of strongly-typed attributes. Attributes \u00b6 Confluo currently supports only bounded-width attributes 1 . An attribute is said to have bounded-width if all records in a single stream use some maximum number of bits to represent that attribute; this includes primitive data types such as binary, integral or floating-point values, or domain-specific types such as IP addresses, ports, sensor readings, etc. Confluo also requires each record in the stream to have a 8-byte nano-second precision timestamp attribute; if the application does not assign timestamps, Confluo internally assigns one during the write operation. Schema \u00b6 A schema in Confluo is a collection of strongly-typed attributes. It is specified via JSON like semantics; for instance, consider the example below for a simple schema with three attributes: { timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) } The first attribute is timestamp with a 8-byte signed integer type; the second, third and fourth attributes correspond to operation latency (in ms), CPU utilization and available memory respectively, all with double precision floating-point type. The final attribute is a log message, with a string type upper bound by 100 characters. Note that each of the attributes must have types associated with them, and each record in a stream with this schema must have its attributes in this order. While Confluo natively supports common primitive types, you can add custom bounded-width data types to Confluo's type system. More details can be found at the Confluo Type-System guide. Atomic MultiLog \u00b6 Atomic MultiLogs are the basic storage abstraction in Confluo, and are similar in interface to database tables. In order to store data from different streams, applications can create an Atomic MultiLog with a pre-specified schema, and write data streams that conform to the schema to the Atomic MultiLog. To support queries, applications can add an index for individual attributes in the schema. Confluo also employs a match-action language with three main elements: filter , aggregate and trigger . A Confluo filter is an expression comprising of relational and boolean operators (see Table below) over arbitrary subset of bounded-width attributes, and identifies records that match the expression. A Confluo aggregate evaluates a computable function on an attribute for all records that match a certain filter expression. Finally, a Confluo trigger is a boolean conditional (e.g., <, >, =, etc.) evaluated over a Confluo aggregate. Relational Operators in Filters : Operator Examples Equality dst_port=80 Range cpu_util>0.8 Boolean Operators in Filters : Operator Examples Conjunction volt>200 && temp>100 Disjunction cpu_util>0.8 || mem_avail<0.1 Negation transport_protocol != TCP Confluo supports indexes, filters, aggregates and triggers only on bounded-width attributes in the schema. Once added, each of these are evaluated and updated upon arrival of each new batch of data records. A Performance Monitoring and Diagnosis Example \u00b6 We will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them, and finally load some data into them, for both embedded and stand-alone modes of operation. We will work with the example of a performance monitoring and diagnosis tool using Confluo. Embedded mode \u00b6 In order to use Confluo in the embedded mode, we simply need to include Confluo's header files under libconfluo/confluo, use the Confluo C++ API in a C++ application, and compile using a modern C++ compiler. The entry point header file to include is confluo_store.h . Creating a New Confluo Store \u00b6 We will first create a new Confluo Store with the data path for it to use as follows: confluo::confluo_store store(\"/path/to/data\"); Creating a New Atomic MultiLog \u00b6 We then create a new Atomic MultiLog within the Store (synonymous to a database table); this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: std::string schema = \"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; store.create_atomic_multilog(\"perf_log\", schema, storage_mode); Our Atomic MultiLog adopts the same schema outlined above . The storage mode is set to in-memory, but can be of the following types: Storage Mode Description IN_MEMORY All data is written purely in memory, and no attempt is made at persisting data to secondary storage. DURABLE Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage. DURABLE_RELAXED Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance. We then obtain a reference to our newly created Atomic MultiLog: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\"); Adding Indexes \u00b6 We can define indexes on the Atomic MultiLog as follows: mlog->add_index(\"op_latency_ms\"); to add an index on op_latency_ms attribute. Adding Filters \u00b6 We can also install filters as follows: mlog->add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); to explicitly filter out records that indicate low system resources (CPU utilization > 80%, Available Memory < 10%), using a filter named low_resources . Adding Aggregates \u00b6 Additionally, we can add aggregates on filters as follows: mlog->add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); This adds a new stored aggregate max_latency_ms on the filter low_resources we defined before. In essence, it records the highest operation latency reported in any record that also indicated low available resources. Installing Triggers \u00b6 Finally, we can install a trigger on aggregates as follows: mlog->install_trigger(\"high_latency_trigger\", \"max_latency > 1000\"); This installs a trigger high_latency_trigger on the aggregate max_latency_ms , which should generate an alert whenever the condition max_latency_ms > 1000 is satisfied, i.e., whenever the maximum latency for an operation exceeds 1s and the available resources are low. Loading sample data into Atomic MultiLog \u00b6 We are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs only support addition of new data via appends . However, new data can be appended in several ways: Appending String Vectors \u00b6 This version of append method takes a vector of strings as its input, where the vector corresponds to a single record. The number of entries in the vector must match the number of entries in the schema, with the exception of the timestamp --- if the timestamp is not provided, Confluo will automatically assign one. size_t off1 = mlog->append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = mlog->append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = mlog->append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); Also note that the operation returns a unique offset corresponding to each append operation. This forms the \"key\" for records stored in the Atomic MultiLog -- records can be retrieved by specifying their corresponding offsets. Appending Raw bytes \u00b6 This version of append takes as its input a pointer to a C/C++ struct, that maps exactly to the Atomic MultiLog's schema. For instance, our schema would map to the following C/C++ struct: struct perf_log_record { int64_t timestamp; double op_latency_ms; double cpu_util; double mem_avail; char log_msg[100]; }; Note that log_msg maps to a char[100] rather than an std::string . To add a new record, we would populate a struct instance, and pass its reference to the append function: int64_t ts = utils::time_utils::cur_ns(); perf_log_record rec = { ts, 2000.0, 0.95, 0.01, \"WARN: Server {2, 4, 5} down\" }; size_t off4 = mlog->append(&rec); Note that this is a more efficient variant of append, since it avoids the overheads of parsing strings to the corresponding attribute data types. Batched Appends \u00b6 It is also possible to batch multiple record appends into a single append. The first step in building a batch is to obtain a batch builder: auto batch_bldr = mlog->get_batch_builder(); The batch builder supports adding new records via both string vector and raw byte interfaces: batch_bldr.add_record({ \"400\", \"0.85\", \"0.07\", \"WARN: Server {2, 4} down\"}); perf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25, \"WARN: Server {2} down\" }; batch_bldr.add_record(&rec); Once the batch is populated, we can append the batch to the Atomic MultiLog as follows: size_t off5 = mlog->append_batch(batch_bldr.get_batch()); To understand how we can query the data we have loaded so far, read the guide on Confluo Queries . Stand-alone Mode \u00b6 In the stand-alone mode, Confluo runs as a daemon server, serving client requests using Apache Thrift protocol. To start the server, run: confluod --address=127.0.0.1 --port=9090 Once the server daemon is running, you can send requests to it using the C++/Python/Java client APIs. Note that the C++ Client API is almost identical to the embedded mode API. We look at the same performance monitoring and diagnosis tool example for the stand-alone mode. The relevant header file to include for the C++ Client API is rpc_client.h . Creating a Client Connection \u00b6 To begin with, we first have to establish a client connection with the server. ```cpp tab=\"C++\" confluo::rpc::rpc_client client(\"127.0.0.1\", 9090); ```python tab=\"Python\" from confluo.rpc.client import RpcClient client = RpcClient(\"127.0.0.1\", 9090) The first argument to the rpc_client constructor corresponds to the server hostname, while the second argument corresponds to the server port. Creating a New Atomic MultiLog \u00b6 We then create a new Atomic MultiLog within the Store (synonymous to a database table); as before, this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: ```cpp tab=\"C++\" std::string schema = \"{ timestamp: LONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; client.create_atomic_multilog(\"perf_log\", schema, storage_mode); ```python tab=\"Python\" from confluo.rpc.storage import StorageMode schema = \"\"\"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"\"\" storage_mode = StorageMode.IN_MEMORY client.create_atomic_multilog(\"perf_log\", schema, storage_mode) This operation also internally sets the current Atomic MultiLog for the client to the one we just created (i.e., perf_log ). It is also possible to explicitly set the current Atomic MultiLog for the client as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\") Note It is necessary to set the current Atomic MultiLog for the rpc_client . Issuing requests via the client without setting the current Atomic MultiLog will result in exceptions. Adding Indexes \u00b6 We can define indexes as follows: ```cpp tab=\"C++\" client.add_index(\"op_latency_ms\"); ```python tab=\"Python\" client.add_index(\"op_latency_ms\") Adding Filters \u00b6 We can also install filters as follows: ```cpp tab=\"C++\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); ```python tab=\"Python\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\") Adding Aggregates \u00b6 Additionally, we can add aggregates on filters as follows: ```cpp tab=\"C++\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); ```python tab=\"Python\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\") Installing Triggers \u00b6 Finally, we can install a trigger on an aggregate as follows: ```cpp tab=\"C++\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\"); ```python tab=\"Python\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\") Loading sample data into Atomic MultiLog \u00b6 We are now ready to load some data into the Atomic MultiLog on the server. Appending String Vectors \u00b6 ```cpp tab=\"C++\" size_t off1 = client.append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = client.append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = client.append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); ```python tab=\"Python\" off1 = client.append([100.0, 0.5, 0.9, \"INFO: Launched 1 tasks\"]) off2 = client.append([500.0, 0.9, 0.05, \"WARN: Server {2} down\"]) off3 = client.append([1001.0, 0.9, 0.03, \"WARN: Server {2, 4, 5} down\"]) Batched Appends \u00b6 It is also possible to batch multiple record appends into a single append operation via the client API. This is particularly useful since batching helps amortize the cost of network latency. The first step in building a batch is to obtain a batch builder: ```cpp tab=\"C++\" auto batch_bldr = client.get_batch_builder(); ```python tab=\"Python\" batch_bldr = client.get_batch_builder() The batch builder supports adding new records via both string vector and raw byte interfaces: ```cpp tab=\"C++\" batch_bldr.add_record({ \"400\", \"0.85\", \"0.07\", \"WARN: Server {2, 4} down\"}); batch_bldr.add_record({ \"100\", \"0.65\", \"0.25\", \"WARN: Server {2} down\" }); ```python tab=\"Python\" batch_bldr.add_record([ 400.0, 0.85, 0.07, \"WARN: Server {2, 4} down\" ]) batch_bldr.add_record([ 100.0, 0.65, 0.25, \"WARN: Server {2} down\" ]) Once the batch is populated, we can append the batch as follows: ```cpp tab=\"C++\" size_t off4 = client.append_batch(batch_bldr.get_batch()); ```python tab=\"Python\" off4 = client.append_batch(batch_bldr.get_batch()) Details on querying the data via the client interface can be found in the guide on Confluo Queries . We plan on adding support for variable width data types in a future relase. \u21a9","title":"Data Storage"},{"location":"loading_data/#data-storage","text":"Confluo operates on data streams . Each stream comprises of records, each of which follows a pre-defined schema over a collection of strongly-typed attributes.","title":"Data Storage"},{"location":"loading_data/#attributes","text":"Confluo currently supports only bounded-width attributes 1 . An attribute is said to have bounded-width if all records in a single stream use some maximum number of bits to represent that attribute; this includes primitive data types such as binary, integral or floating-point values, or domain-specific types such as IP addresses, ports, sensor readings, etc. Confluo also requires each record in the stream to have a 8-byte nano-second precision timestamp attribute; if the application does not assign timestamps, Confluo internally assigns one during the write operation.","title":"Attributes"},{"location":"loading_data/#schema","text":"A schema in Confluo is a collection of strongly-typed attributes. It is specified via JSON like semantics; for instance, consider the example below for a simple schema with three attributes: { timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) } The first attribute is timestamp with a 8-byte signed integer type; the second, third and fourth attributes correspond to operation latency (in ms), CPU utilization and available memory respectively, all with double precision floating-point type. The final attribute is a log message, with a string type upper bound by 100 characters. Note that each of the attributes must have types associated with them, and each record in a stream with this schema must have its attributes in this order. While Confluo natively supports common primitive types, you can add custom bounded-width data types to Confluo's type system. More details can be found at the Confluo Type-System guide.","title":"Schema"},{"location":"loading_data/#atomic-multilog","text":"Atomic MultiLogs are the basic storage abstraction in Confluo, and are similar in interface to database tables. In order to store data from different streams, applications can create an Atomic MultiLog with a pre-specified schema, and write data streams that conform to the schema to the Atomic MultiLog. To support queries, applications can add an index for individual attributes in the schema. Confluo also employs a match-action language with three main elements: filter , aggregate and trigger . A Confluo filter is an expression comprising of relational and boolean operators (see Table below) over arbitrary subset of bounded-width attributes, and identifies records that match the expression. A Confluo aggregate evaluates a computable function on an attribute for all records that match a certain filter expression. Finally, a Confluo trigger is a boolean conditional (e.g., <, >, =, etc.) evaluated over a Confluo aggregate. Relational Operators in Filters : Operator Examples Equality dst_port=80 Range cpu_util>0.8 Boolean Operators in Filters : Operator Examples Conjunction volt>200 && temp>100 Disjunction cpu_util>0.8 || mem_avail<0.1 Negation transport_protocol != TCP Confluo supports indexes, filters, aggregates and triggers only on bounded-width attributes in the schema. Once added, each of these are evaluated and updated upon arrival of each new batch of data records.","title":"Atomic MultiLog"},{"location":"loading_data/#a-performance-monitoring-and-diagnosis-example","text":"We will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them, and finally load some data into them, for both embedded and stand-alone modes of operation. We will work with the example of a performance monitoring and diagnosis tool using Confluo.","title":"A Performance Monitoring and Diagnosis Example"},{"location":"loading_data/#embedded-mode","text":"In order to use Confluo in the embedded mode, we simply need to include Confluo's header files under libconfluo/confluo, use the Confluo C++ API in a C++ application, and compile using a modern C++ compiler. The entry point header file to include is confluo_store.h .","title":"Embedded mode"},{"location":"loading_data/#creating-a-new-confluo-store","text":"We will first create a new Confluo Store with the data path for it to use as follows: confluo::confluo_store store(\"/path/to/data\");","title":"Creating a New Confluo Store"},{"location":"loading_data/#creating-a-new-atomic-multilog","text":"We then create a new Atomic MultiLog within the Store (synonymous to a database table); this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: std::string schema = \"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; store.create_atomic_multilog(\"perf_log\", schema, storage_mode); Our Atomic MultiLog adopts the same schema outlined above . The storage mode is set to in-memory, but can be of the following types: Storage Mode Description IN_MEMORY All data is written purely in memory, and no attempt is made at persisting data to secondary storage. DURABLE Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage. DURABLE_RELAXED Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance. We then obtain a reference to our newly created Atomic MultiLog: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\");","title":"Creating a New Atomic MultiLog"},{"location":"loading_data/#adding-indexes","text":"We can define indexes on the Atomic MultiLog as follows: mlog->add_index(\"op_latency_ms\"); to add an index on op_latency_ms attribute.","title":"Adding Indexes"},{"location":"loading_data/#adding-filters","text":"We can also install filters as follows: mlog->add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); to explicitly filter out records that indicate low system resources (CPU utilization > 80%, Available Memory < 10%), using a filter named low_resources .","title":"Adding Filters"},{"location":"loading_data/#adding-aggregates","text":"Additionally, we can add aggregates on filters as follows: mlog->add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); This adds a new stored aggregate max_latency_ms on the filter low_resources we defined before. In essence, it records the highest operation latency reported in any record that also indicated low available resources.","title":"Adding Aggregates"},{"location":"loading_data/#installing-triggers","text":"Finally, we can install a trigger on aggregates as follows: mlog->install_trigger(\"high_latency_trigger\", \"max_latency > 1000\"); This installs a trigger high_latency_trigger on the aggregate max_latency_ms , which should generate an alert whenever the condition max_latency_ms > 1000 is satisfied, i.e., whenever the maximum latency for an operation exceeds 1s and the available resources are low.","title":"Installing Triggers"},{"location":"loading_data/#loading-sample-data-into-atomic-multilog","text":"We are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs only support addition of new data via appends . However, new data can be appended in several ways:","title":"Loading sample data into Atomic MultiLog"},{"location":"loading_data/#appending-string-vectors","text":"This version of append method takes a vector of strings as its input, where the vector corresponds to a single record. The number of entries in the vector must match the number of entries in the schema, with the exception of the timestamp --- if the timestamp is not provided, Confluo will automatically assign one. size_t off1 = mlog->append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = mlog->append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = mlog->append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); Also note that the operation returns a unique offset corresponding to each append operation. This forms the \"key\" for records stored in the Atomic MultiLog -- records can be retrieved by specifying their corresponding offsets.","title":"Appending String Vectors"},{"location":"loading_data/#appending-raw-bytes","text":"This version of append takes as its input a pointer to a C/C++ struct, that maps exactly to the Atomic MultiLog's schema. For instance, our schema would map to the following C/C++ struct: struct perf_log_record { int64_t timestamp; double op_latency_ms; double cpu_util; double mem_avail; char log_msg[100]; }; Note that log_msg maps to a char[100] rather than an std::string . To add a new record, we would populate a struct instance, and pass its reference to the append function: int64_t ts = utils::time_utils::cur_ns(); perf_log_record rec = { ts, 2000.0, 0.95, 0.01, \"WARN: Server {2, 4, 5} down\" }; size_t off4 = mlog->append(&rec); Note that this is a more efficient variant of append, since it avoids the overheads of parsing strings to the corresponding attribute data types.","title":"Appending Raw bytes"},{"location":"loading_data/#batched-appends","text":"It is also possible to batch multiple record appends into a single append. The first step in building a batch is to obtain a batch builder: auto batch_bldr = mlog->get_batch_builder(); The batch builder supports adding new records via both string vector and raw byte interfaces: batch_bldr.add_record({ \"400\", \"0.85\", \"0.07\", \"WARN: Server {2, 4} down\"}); perf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25, \"WARN: Server {2} down\" }; batch_bldr.add_record(&rec); Once the batch is populated, we can append the batch to the Atomic MultiLog as follows: size_t off5 = mlog->append_batch(batch_bldr.get_batch()); To understand how we can query the data we have loaded so far, read the guide on Confluo Queries .","title":"Batched Appends"},{"location":"loading_data/#stand-alone-mode","text":"In the stand-alone mode, Confluo runs as a daemon server, serving client requests using Apache Thrift protocol. To start the server, run: confluod --address=127.0.0.1 --port=9090 Once the server daemon is running, you can send requests to it using the C++/Python/Java client APIs. Note that the C++ Client API is almost identical to the embedded mode API. We look at the same performance monitoring and diagnosis tool example for the stand-alone mode. The relevant header file to include for the C++ Client API is rpc_client.h .","title":"Stand-alone Mode"},{"location":"loading_data/#creating-a-client-connection","text":"To begin with, we first have to establish a client connection with the server. ```cpp tab=\"C++\" confluo::rpc::rpc_client client(\"127.0.0.1\", 9090); ```python tab=\"Python\" from confluo.rpc.client import RpcClient client = RpcClient(\"127.0.0.1\", 9090) The first argument to the rpc_client constructor corresponds to the server hostname, while the second argument corresponds to the server port.","title":"Creating a Client Connection"},{"location":"loading_data/#creating-a-new-atomic-multilog_1","text":"We then create a new Atomic MultiLog within the Store (synonymous to a database table); as before, this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: ```cpp tab=\"C++\" std::string schema = \"{ timestamp: LONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; client.create_atomic_multilog(\"perf_log\", schema, storage_mode); ```python tab=\"Python\" from confluo.rpc.storage import StorageMode schema = \"\"\"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"\"\" storage_mode = StorageMode.IN_MEMORY client.create_atomic_multilog(\"perf_log\", schema, storage_mode) This operation also internally sets the current Atomic MultiLog for the client to the one we just created (i.e., perf_log ). It is also possible to explicitly set the current Atomic MultiLog for the client as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\") Note It is necessary to set the current Atomic MultiLog for the rpc_client . Issuing requests via the client without setting the current Atomic MultiLog will result in exceptions.","title":"Creating a New Atomic MultiLog"},{"location":"loading_data/#adding-indexes_1","text":"We can define indexes as follows: ```cpp tab=\"C++\" client.add_index(\"op_latency_ms\"); ```python tab=\"Python\" client.add_index(\"op_latency_ms\")","title":"Adding Indexes"},{"location":"loading_data/#adding-filters_1","text":"We can also install filters as follows: ```cpp tab=\"C++\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); ```python tab=\"Python\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\")","title":"Adding Filters"},{"location":"loading_data/#adding-aggregates_1","text":"Additionally, we can add aggregates on filters as follows: ```cpp tab=\"C++\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); ```python tab=\"Python\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\")","title":"Adding Aggregates"},{"location":"loading_data/#installing-triggers_1","text":"Finally, we can install a trigger on an aggregate as follows: ```cpp tab=\"C++\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\"); ```python tab=\"Python\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\")","title":"Installing Triggers"},{"location":"loading_data/#loading-sample-data-into-atomic-multilog_1","text":"We are now ready to load some data into the Atomic MultiLog on the server.","title":"Loading sample data into Atomic MultiLog"},{"location":"loading_data/#appending-string-vectors_1","text":"```cpp tab=\"C++\" size_t off1 = client.append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = client.append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = client.append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); ```python tab=\"Python\" off1 = client.append([100.0, 0.5, 0.9, \"INFO: Launched 1 tasks\"]) off2 = client.append([500.0, 0.9, 0.05, \"WARN: Server {2} down\"]) off3 = client.append([1001.0, 0.9, 0.03, \"WARN: Server {2, 4, 5} down\"])","title":"Appending String Vectors"},{"location":"loading_data/#batched-appends_1","text":"It is also possible to batch multiple record appends into a single append operation via the client API. This is particularly useful since batching helps amortize the cost of network latency. The first step in building a batch is to obtain a batch builder: ```cpp tab=\"C++\" auto batch_bldr = client.get_batch_builder(); ```python tab=\"Python\" batch_bldr = client.get_batch_builder() The batch builder supports adding new records via both string vector and raw byte interfaces: ```cpp tab=\"C++\" batch_bldr.add_record({ \"400\", \"0.85\", \"0.07\", \"WARN: Server {2, 4} down\"}); batch_bldr.add_record({ \"100\", \"0.65\", \"0.25\", \"WARN: Server {2} down\" }); ```python tab=\"Python\" batch_bldr.add_record([ 400.0, 0.85, 0.07, \"WARN: Server {2, 4} down\" ]) batch_bldr.add_record([ 100.0, 0.65, 0.25, \"WARN: Server {2} down\" ]) Once the batch is populated, we can append the batch as follows: ```cpp tab=\"C++\" size_t off4 = client.append_batch(batch_bldr.get_batch()); ```python tab=\"Python\" off4 = client.append_batch(batch_bldr.get_batch()) Details on querying the data via the client interface can be found in the guide on Confluo Queries . We plan on adding support for variable width data types in a future relase. \u21a9","title":"Batched Appends"},{"location":"modes_of_operation/","text":"Modes of Operation \u00b6 Confluo can be used in two modes -- embedded and stand-alone. Embedded Mode \u00b6 In the embedded mode , Confluo is used as a header-only library in C++, allowing Confluo to use the same address-space as the application process. This enables ultra low-latency writes and queries, but only supports applications written in C++. Stand-alone Mode \u00b6 Confluo also supports a stand-alone mode , where Confluo runs as a daemon server process and allows clients to communicate with it using Apache Thrift protocol. Operations now incur higher latencies (due to serialization/deserialization overheads), but can now operate over the network, and allows Confluo to store data from applications written in different languages. More on Usage \u00b6 Read more on how you can perform different operations with the two modes of operation: Data Storage and Loading Data Querying Data Online Queries Offline Queries","title":"Modes of Operation"},{"location":"modes_of_operation/#modes-of-operation","text":"Confluo can be used in two modes -- embedded and stand-alone.","title":"Modes of Operation"},{"location":"modes_of_operation/#embedded-mode","text":"In the embedded mode , Confluo is used as a header-only library in C++, allowing Confluo to use the same address-space as the application process. This enables ultra low-latency writes and queries, but only supports applications written in C++.","title":"Embedded Mode"},{"location":"modes_of_operation/#stand-alone-mode","text":"Confluo also supports a stand-alone mode , where Confluo runs as a daemon server process and allows clients to communicate with it using Apache Thrift protocol. Operations now incur higher latencies (due to serialization/deserialization overheads), but can now operate over the network, and allows Confluo to store data from applications written in different languages.","title":"Stand-alone Mode"},{"location":"modes_of_operation/#more-on-usage","text":"Read more on how you can perform different operations with the two modes of operation: Data Storage and Loading Data Querying Data Online Queries Offline Queries","title":"More on Usage"},{"location":"network_monitoring/","text":"Network Monitoring and Diagnosis \u00b6 Network monitoring and diagnosis is an increasingly challenging task for network operators. Integrating these functionalities within the network stack at the end-hosts allows efficiently using end-host programmability and resources with minimal overheads (end-host stack process the incoming packets anyway). However, achieving this requires tools that support highly concurrent per-packet capture at line-rate, support for online queries (for monitoring purposes), and offline queries (for diagnosis purposes). Confluo interface is a natural fit for building such a tool -- flows, packets headers and header fields at an end-host map perfectly to Confluo streams, records and attributes. For details on the design, implementation and evaluation of the network monitoring and diagnosis tool, please see our NSDI paper .","title":"Network Monitoring and Diagnosis"},{"location":"network_monitoring/#network-monitoring-and-diagnosis","text":"Network monitoring and diagnosis is an increasingly challenging task for network operators. Integrating these functionalities within the network stack at the end-hosts allows efficiently using end-host programmability and resources with minimal overheads (end-host stack process the incoming packets anyway). However, achieving this requires tools that support highly concurrent per-packet capture at line-rate, support for online queries (for monitoring purposes), and offline queries (for diagnosis purposes). Confluo interface is a natural fit for building such a tool -- flows, packets headers and header fields at an end-host map perfectly to Confluo streams, records and attributes. For details on the design, implementation and evaluation of the network monitoring and diagnosis tool, please see our NSDI paper .","title":"Network Monitoring and Diagnosis"},{"location":"offline_queries/","text":"Offline Queries \u00b6 Offline queries in Confluo are evaluated during runtime, i.e., they are executed on-the-fly on already written data in response to user requests. The execution of these queries rely on the raw data and attribute indexes. The guide on Data Storage describes how attribute indexes can be added to Atomic MultiLogs. In this guide, we will focus on how we can query this data. Embedded Mode \u00b6 We start with Confluo's embedded mode of operation. We work with the assumption that the Atomic MultiLog has already been created, and filters, aggregates and triggers have been added to it as outlined in Data Storage . To start with, we have a reference to the Atomic MultiLog as follows: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\"); Retrieving Records \u00b6 It is straightforward to retrieve records given their offsets: auto record1 = mlog->read(off1); auto record2 = mlog->read(off2); auto record3 = mlog->read(off3); Each of record1 , record2 , and record3 are vectors of strings. Evaluating Ad-hoc Filter Expressions \u00b6 We can query indexed attributes as follows: auto record_stream = mlog->execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes as its argument a filter expression; see the guide on Data Storage for details on the elements of the filter expression. The operation returns a lazily evaluated stream, which supports functional style operations like map, filter, etc. See Stream API for more details. Stand-alone Mode \u00b6 The API for Stand-alone mode of operation is quite similar to the embedded mode. We only focus on the C++ Client API, since Python and Java Client APIs are almost identical to the C++ Client API. As with the embedded mode, we work with the assumption that the client is connected to the server, has already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers. Also, the current Atomic MultiLog for the client has been set to perf_log as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\") Retrieving Records \u00b6 It is straightforward to retrieve records given their offsets: ```cpp tab=\"C++\" auto record1 = client.read(off1); auto record2 = client.read(off2); auto record3 = client.read(off3); ```python tab=\"Python\" record1 = client.read(off1) record2 = client.read(off2) record3 = client.read(off3) Evaluating Ad-hoc Filter Expressions \u00b6 We can query indexed attributes as follows: ```cpp tab=\"C++\" auto record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\") for r in record_stream: print r This operation returns a lazy stream of records, which automatically fetches more data from the server as the clients consumes them.","title":"Offline Queries"},{"location":"offline_queries/#offline-queries","text":"Offline queries in Confluo are evaluated during runtime, i.e., they are executed on-the-fly on already written data in response to user requests. The execution of these queries rely on the raw data and attribute indexes. The guide on Data Storage describes how attribute indexes can be added to Atomic MultiLogs. In this guide, we will focus on how we can query this data.","title":"Offline Queries"},{"location":"offline_queries/#embedded-mode","text":"We start with Confluo's embedded mode of operation. We work with the assumption that the Atomic MultiLog has already been created, and filters, aggregates and triggers have been added to it as outlined in Data Storage . To start with, we have a reference to the Atomic MultiLog as follows: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\");","title":"Embedded Mode"},{"location":"offline_queries/#retrieving-records","text":"It is straightforward to retrieve records given their offsets: auto record1 = mlog->read(off1); auto record2 = mlog->read(off2); auto record3 = mlog->read(off3); Each of record1 , record2 , and record3 are vectors of strings.","title":"Retrieving Records"},{"location":"offline_queries/#evaluating-ad-hoc-filter-expressions","text":"We can query indexed attributes as follows: auto record_stream = mlog->execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes as its argument a filter expression; see the guide on Data Storage for details on the elements of the filter expression. The operation returns a lazily evaluated stream, which supports functional style operations like map, filter, etc. See Stream API for more details.","title":"Evaluating Ad-hoc Filter Expressions"},{"location":"offline_queries/#stand-alone-mode","text":"The API for Stand-alone mode of operation is quite similar to the embedded mode. We only focus on the C++ Client API, since Python and Java Client APIs are almost identical to the C++ Client API. As with the embedded mode, we work with the assumption that the client is connected to the server, has already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers. Also, the current Atomic MultiLog for the client has been set to perf_log as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\")","title":"Stand-alone Mode"},{"location":"offline_queries/#retrieving-records_1","text":"It is straightforward to retrieve records given their offsets: ```cpp tab=\"C++\" auto record1 = client.read(off1); auto record2 = client.read(off2); auto record3 = client.read(off3); ```python tab=\"Python\" record1 = client.read(off1) record2 = client.read(off2) record3 = client.read(off3)","title":"Retrieving Records"},{"location":"offline_queries/#evaluating-ad-hoc-filter-expressions_1","text":"We can query indexed attributes as follows: ```cpp tab=\"C++\" auto record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\") for r in record_stream: print r This operation returns a lazy stream of records, which automatically fetches more data from the server as the clients consumes them.","title":"Evaluating Ad-hoc Filter Expressions"},{"location":"online_queries/","text":"Online Queries \u00b6 Online queries on Confluo are executed automatically as new records are written into an Atomic MultiLog. Therefore, these queries need to be pre-defined; the guide on Data Storage describes how we can define filters, aggregates and triggers for online evaluation. In this guide, we will look at how we can retrieve the results of these online queries. Embedded Mode \u00b6 We begin with the embedded mode of operation. We work with the assumption that the Atomic MultiLog has already been created, and filters, aggregates and triggers have been added to it as outlined in Data Storage . To start with, we have a reference to the Atomic MultiLog as follows: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\"); Querying Pre-defined Filters \u00b6 We can query a pre-defined filter as follows: auto record_stream = mlog->query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The first parameter corresponds to the name of the filter to be queried, while the second and third parameters correspond to the begining timestamp and end timestamp to consider for records in the filter. We've specified them to capture all possible values of timestamp. This operation returns a lazily evaluated record stream which supports functional semantics such as filter, map, etc. See Confluo's Stream API for more details on how to work with lazy streams. Obtaining Pre-defined Aggregates \u00b6 We can obtian a pre-defined aggregate as follows: auto value = mlog->get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value.to_string(); The operation takes the name of the aggregate as its first parameter, while the second and third parameters correspond to begin and end timestmaps, as with pre-defined filters. The query returns a numeric object, which is a wrapper around numeric values in C++. Obtaining Alerts from a Pre-defined Trigger \u00b6 Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: auto alert_stream = mlog->get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes begin and end timestamps as its first and second arguments, and an optional trigger name as its third argument. The query returns a lazy stream over generated alerts for this trigger in the specified time-range. Stand-alone Mode \u00b6 The API for Stand-alone mode of operation is quite similar to the embedded mode. We only focus on the C++ Client API, since Python and Java Client APIs are almost identical to the C++ Client API. As with the embedded mode, we work with the assumption that the client is connected to the server, has already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers. Also, the current Atomic MultiLog for the client has been set to perf_log as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\") Querying Pre-defined Filters \u00b6 We can query a pre-defined filter as follows: ```cpp tab=\"C++\" auto record_stream = client.query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" import sys record_stream = client.query_filter(\"low_resources\", 0, sys.maxsize); for r in record_stream: print r This operation returns a lazy stream of records, which automatically fetches more data from the server as the clients consumes them. Obtaining Pre-defined Aggregates \u00b6 We can obtian a pre-defined aggregate as follows: ```cpp tab=\"C++\" std::string value = client.get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value; ```python tab=\"Python\" import sys value = client.get_aggregate(\"max_latency_ms\", 0, sys.maxint) print value The operation returns a string representation of the aggregate. Obtaining Alerts from a Pre-defined Trigger \u00b6 Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: ```cpp tab=\"C++\" auto alert_stream = client.get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); ++s) { std::cout << s.get(); } ```cpp tab=\"Python\" import sys alert_stream = client.get_alerts(0, sys.maxint, \"high_latency_trigger\") for a in alert_stream: print a Similar to the filter query, this operation returns a lazy stream of alerts, which automatically fetches more data from the server as the clients consumes them.","title":"Online Queries"},{"location":"online_queries/#online-queries","text":"Online queries on Confluo are executed automatically as new records are written into an Atomic MultiLog. Therefore, these queries need to be pre-defined; the guide on Data Storage describes how we can define filters, aggregates and triggers for online evaluation. In this guide, we will look at how we can retrieve the results of these online queries.","title":"Online Queries"},{"location":"online_queries/#embedded-mode","text":"We begin with the embedded mode of operation. We work with the assumption that the Atomic MultiLog has already been created, and filters, aggregates and triggers have been added to it as outlined in Data Storage . To start with, we have a reference to the Atomic MultiLog as follows: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\");","title":"Embedded Mode"},{"location":"online_queries/#querying-pre-defined-filters","text":"We can query a pre-defined filter as follows: auto record_stream = mlog->query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The first parameter corresponds to the name of the filter to be queried, while the second and third parameters correspond to the begining timestamp and end timestamp to consider for records in the filter. We've specified them to capture all possible values of timestamp. This operation returns a lazily evaluated record stream which supports functional semantics such as filter, map, etc. See Confluo's Stream API for more details on how to work with lazy streams.","title":"Querying Pre-defined Filters"},{"location":"online_queries/#obtaining-pre-defined-aggregates","text":"We can obtian a pre-defined aggregate as follows: auto value = mlog->get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value.to_string(); The operation takes the name of the aggregate as its first parameter, while the second and third parameters correspond to begin and end timestmaps, as with pre-defined filters. The query returns a numeric object, which is a wrapper around numeric values in C++.","title":"Obtaining Pre-defined Aggregates"},{"location":"online_queries/#obtaining-alerts-from-a-pre-defined-trigger","text":"Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: auto alert_stream = mlog->get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes begin and end timestamps as its first and second arguments, and an optional trigger name as its third argument. The query returns a lazy stream over generated alerts for this trigger in the specified time-range.","title":"Obtaining Alerts from a Pre-defined Trigger"},{"location":"online_queries/#stand-alone-mode","text":"The API for Stand-alone mode of operation is quite similar to the embedded mode. We only focus on the C++ Client API, since Python and Java Client APIs are almost identical to the C++ Client API. As with the embedded mode, we work with the assumption that the client is connected to the server, has already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers. Also, the current Atomic MultiLog for the client has been set to perf_log as follows: ```cpp tab=\"C++\" client.set_current_atomic_multilog(\"perf_log\"); ```python tab=\"Python\" client.set_current_atomic_multilog(\"perf_log\")","title":"Stand-alone Mode"},{"location":"online_queries/#querying-pre-defined-filters_1","text":"We can query a pre-defined filter as follows: ```cpp tab=\"C++\" auto record_stream = client.query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" import sys record_stream = client.query_filter(\"low_resources\", 0, sys.maxsize); for r in record_stream: print r This operation returns a lazy stream of records, which automatically fetches more data from the server as the clients consumes them.","title":"Querying Pre-defined Filters"},{"location":"online_queries/#obtaining-pre-defined-aggregates_1","text":"We can obtian a pre-defined aggregate as follows: ```cpp tab=\"C++\" std::string value = client.get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value; ```python tab=\"Python\" import sys value = client.get_aggregate(\"max_latency_ms\", 0, sys.maxint) print value The operation returns a string representation of the aggregate.","title":"Obtaining Pre-defined Aggregates"},{"location":"online_queries/#obtaining-alerts-from-a-pre-defined-trigger_1","text":"Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: ```cpp tab=\"C++\" auto alert_stream = client.get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); ++s) { std::cout << s.get(); } ```cpp tab=\"Python\" import sys alert_stream = client.get_alerts(0, sys.maxint, \"high_latency_trigger\") for a in alert_stream: print a Similar to the filter query, this operation returns a lazy stream of alerts, which automatically fetches more data from the server as the clients consumes them.","title":"Obtaining Alerts from a Pre-defined Trigger"},{"location":"pub_sub/","text":"Pub-sub System \u00b6 Pub-sub systems like Kafka and Kinesis expose a publish-subscribe interface atop partitioned logs. We describe the implementation of a pub-sub system using Confluo that enables high publish and subscribe throughput for messages via lock-free concurrency. Implementation \u00b6 Our distributed messaging system implementation employs Kafka\u2019s interface and data model \u2014 messages are published to or subscribed from \"topics \", which are logically streams of messages. The system maintains a collection of topics, where messages for each topic are stored across a user-specified number of Confluo shards. In our implementation, each shard exposes a basic read and write interface. The publishers write messages in batches to shards of a particular topic, while subscribers asynchronously pull batches of messages from these shards. Similar to Kafka design, each subscriber keeps track of the objectId for its last read message in the shard, incrementing it as it consumes more messages. The key benefits of using Confluo for storing messages include: The freedom from read-write contentions, and lock-free resolution of write-write contentions. Confluo provides an efficient means to obtain the snapshot of an entire topic, unlike Kafka. Support for rich online and offline queries on message streams beyond just publish and subscribe. Compared Systems and Experimental Setup \u00b6 We compare the performance for our pub-sub implementation against Apache Kafka. Since both systems are identical in terms of scaling read and write performance via multiple partitions, we ran our experiments on a single r3.8xlarge instance, using a single topic with one log partition for both systems. Reads and writes were performed for 64 byte messages, and concurrent subscribers in both systems belong to different subscriber groups, i.e., perform independent, uncoordinated reads on the partition. We mount Kafka\u2019s storage on a sufficiently sized RAM disk, ensuring that both systems operate completely in memory. Results \u00b6 Figure: Confluo observes close to linear write throughput scaling with #publishers as opposed to Kafka\u2019s sub-linear scaling, while both systems observe close to linear read throughput scaling with #subscribers (both axes are in log-scale). Since Kafka employs locks to synchronize concurrent appends, publisher write throughput suffers due to write-write contentions (Figure (left)). Confluo employs lock-free resolution for these conflicts to achieve high write throughput. Larger batches (16K messages) alleviate locking overheads in Kafka to some extent, while Confluo approaches network saturation at 16K message batches with over 4 publishers. Since reads occur without contention in both systems, read throughput scales linearly with multiple subscribers (Figure (right)). Confluo achieves higher absolute read throughput, presumably due to system overheads in Kafka and not because of a fundamental design difference. As before, read throughput for Confluo saturates at 4 subscribers and 16K message batches due to network saturation.","title":"Pub-sub System"},{"location":"pub_sub/#pub-sub-system","text":"Pub-sub systems like Kafka and Kinesis expose a publish-subscribe interface atop partitioned logs. We describe the implementation of a pub-sub system using Confluo that enables high publish and subscribe throughput for messages via lock-free concurrency.","title":"Pub-sub System"},{"location":"pub_sub/#implementation","text":"Our distributed messaging system implementation employs Kafka\u2019s interface and data model \u2014 messages are published to or subscribed from \"topics \", which are logically streams of messages. The system maintains a collection of topics, where messages for each topic are stored across a user-specified number of Confluo shards. In our implementation, each shard exposes a basic read and write interface. The publishers write messages in batches to shards of a particular topic, while subscribers asynchronously pull batches of messages from these shards. Similar to Kafka design, each subscriber keeps track of the objectId for its last read message in the shard, incrementing it as it consumes more messages. The key benefits of using Confluo for storing messages include: The freedom from read-write contentions, and lock-free resolution of write-write contentions. Confluo provides an efficient means to obtain the snapshot of an entire topic, unlike Kafka. Support for rich online and offline queries on message streams beyond just publish and subscribe.","title":"Implementation"},{"location":"pub_sub/#compared-systems-and-experimental-setup","text":"We compare the performance for our pub-sub implementation against Apache Kafka. Since both systems are identical in terms of scaling read and write performance via multiple partitions, we ran our experiments on a single r3.8xlarge instance, using a single topic with one log partition for both systems. Reads and writes were performed for 64 byte messages, and concurrent subscribers in both systems belong to different subscriber groups, i.e., perform independent, uncoordinated reads on the partition. We mount Kafka\u2019s storage on a sufficiently sized RAM disk, ensuring that both systems operate completely in memory.","title":"Compared Systems and Experimental Setup"},{"location":"pub_sub/#results","text":"Figure: Confluo observes close to linear write throughput scaling with #publishers as opposed to Kafka\u2019s sub-linear scaling, while both systems observe close to linear read throughput scaling with #subscribers (both axes are in log-scale). Since Kafka employs locks to synchronize concurrent appends, publisher write throughput suffers due to write-write contentions (Figure (left)). Confluo employs lock-free resolution for these conflicts to achieve high write throughput. Larger batches (16K messages) alleviate locking overheads in Kafka to some extent, while Confluo approaches network saturation at 16K message batches with over 4 publishers. Since reads occur without contention in both systems, read throughput scales linearly with multiple subscribers (Figure (right)). Confluo achieves higher absolute read throughput, presumably due to system overheads in Kafka and not because of a fundamental design difference. As before, read throughput for Confluo saturates at 4 subscribers and 16K message batches due to network saturation.","title":"Results"},{"location":"python_client_api/","text":"Python Client API Documentation \u00b6 This page will be replaced by API documentation generated by Sphinx.","title":"Python Client API Documentation"},{"location":"python_client_api/#python-client-api-documentation","text":"This page will be replaced by API documentation generated by Sphinx.","title":"Python Client API Documentation"},{"location":"queries/","text":"Querying Data \u00b6 Queries in Confluo can either be online or offline . Online queries are executed as new data records are written to an Atomic MultiLog, while offline queries are evaluated on already written records. In essence, online queries are similar to continuous queries databases. In order to support online and offline queries, Confluo makes use of indexes, filters, aggregates and triggers. The interface for adding these elements to an Atomic MultiLog was described in the guide on Data Storage and Loading . To see how Confluo supports online and offline queries, see the individual guides at: Online Queries Offline Queries","title":"Querying Data"},{"location":"queries/#querying-data","text":"Queries in Confluo can either be online or offline . Online queries are executed as new data records are written to an Atomic MultiLog, while offline queries are evaluated on already written records. In essence, online queries are similar to continuous queries databases. In order to support online and offline queries, Confluo makes use of indexes, filters, aggregates and triggers. The interface for adding these elements to an Atomic MultiLog was described in the guide on Data Storage and Loading . To see how Confluo supports online and offline queries, see the individual guides at: Online Queries Offline Queries","title":"Querying Data"},{"location":"quick_start/","text":"Quick Start \u00b6 In this Quick Start, we will take a look at how to download and setup Confluo, load some sample data, and query it. Pre-requisites \u00b6 MacOS X or Unix-based OS; Windows is not yet supported. C++ compiler that supports C++11 standard (e.g., GCC 5.3 or later) CMake 3.2 or later Boost 1.58 or later For python client, you will additionally require: Python 2.7 or later Python Packages: setuptools, six 1.7.2 or later For java client, you will additionally require: Java JDK 1.7 or later ant 1.6.2 or later Download and Install \u00b6 To download and install Confluo, use the following commands: git clone https://github.com/ucbrise/confluo.git cd confluo mkdir build cd build cmake .. make -j && make test && make install Using Confluo \u00b6 Confluo can be used in two modes -- embedded and stand-alone. In the embedded mode, Confluo is used as a header-only library in C++, allowing Confluo to use the same address-space as the application process. In the stand-alone mode, Confluo runs as a daemon server process, allowing clients to communicate with it using Apache Thrift protocol. Embedded Mode \u00b6 In order to use Confluo in the embedded mode, we simply need to include Confluo's header files under libconfluo/confluo, use the Confluo C++ API in a C++ application, and compile using a modern C++ compiler. The entry point header file to include is confluo_store.h . We will first create a new Confluo Store with the data path for it to use as follows: confluo::confluo_store store(\"/path/to/data\"); We then create a new Atomic MultiLog within the Store (synonymous to a database table); this requires three parameters: name, schema, and the storage mode: std::string schema = \"{ timestamp: LONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; store.create_atomic_multilog(\"perf_log\", schema, storage_mode); Our schema contains 5 attributes: a signed 8-byte integer timestamp, double floating-point precision operation latency (in ms), CPU utilization and available memory, and a string log message field (upper bounded to 100 characters). We then obtain a reference to our newly created Atomic MultiLog: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\"); We can define indexes on the Atomic MultiLog as follows: mlog->add_index(\"op_latency_ms\"); to add an index on op_latency_ms attribute. We can also install filters as follows: mlog->add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); to explicitly filter out records that indicate low system resources (CPU utilization > 80%, Available Memory < 10%), using a filter named low_resources . Additionally, we can add aggregates on filters as follows: mlog->add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); This adds a new stored aggregate max_latency_ms on the filter low_resources we defined before. In essence, it records the highest operation latency reported in any record that also indicated low available resources. Finally, we can install a trigger on aggregates as follows: mlog->install_trigger(\"high_latency_trigger\", \"max_latency > 1000\"); This installs a trigger high_latency_trigger on the aggregate max_latency_ms , which should generate an alert whenever the condition max_latency_ms > 1000 is satisfied, i.e., whenever the maximum latency for an operation exceeds 1s and the available resources are low. We are now ready to load some data into this multilog: size_t off1 = mlog->append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = mlog->append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = mlog->append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); Note that the append method takes a vector of strings as its input, where the vector corresponds to a single record. The number of entries in the vector must match the number of entries in the schema, with the exception of the timestamp --- if the timestamp is not provided, Confluo will automatically assign one. Also note that the operation returns a unique offset corresponding to each append operation. This forms the \"key\" for records stored in the Atomic MultiLog -- records can be retrieved by specifying their corresponding offsets. Now we take a look at how we can query the data in the Atomic MultiLog. First, it is straightforward to retrieve records given their offsets: auto record1 = mlog->read(off1); auto record2 = mlog->read(off2); auto record3 = mlog->read(off3); Each of record1 , record2 , and record3 are vectors of strings. We can query indexed attributes as follows: auto record_stream1 = mlog->execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream1; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } Note that the operation returns a lazily evaluated stream, which supports functional style operations like map, filter, etc. See stream.h for more details. We can also query the defined filter as follows: auto record_stream2 = mlog->query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream2; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The first parameter corresponds to the name of the filter to be queried, while the second and third parameters correspond to the begining timestamp and end timestamp to consider for records in the filter. We've specified them to capture all possible values of timestamp. Similar to the execute_filter query, this operation also returns a lazily evaluated record stream. We query aggregates as follows: auto value = mlog->get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value.to_string(); The query takes the name of the aggregate as its first parameter, while the second and third parameters correspond to begin and end timestmaps, as before. The query returns a numeric object, which is a wrapper around numeric values. Finally, we can query the generated alerts by triggers we have installed as follows: auto alert_stream = mlog->get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes and begin and end timestamps as its first and second arguments, and an optional trigger name as its third argument. The query returns a lazy stream over generated alerts for this trigger in the specified time-range. See API docs for C++ and in-depth user guides on Data Storage and Conflo Queries for details on Confluo's supported operations. Stand-alone Mode \u00b6 In the stand-alone mode, Confluo runs as a daemon server, serving client requests using Apache Thrift protocol. To start the server, run: confluod --address=127.0.0.1 --port=9090 Once the server daemon is running, you can query it using the C++, Python or Java client APIs. The client APIs closely resemble the embedded API. We first create a new client connection to the Confluo daemon: ```cpp tab=\"C++\" confluo::rpc::rpc_client client(\"127.0.0.1\", 9090); ```python tab=\"Python\" from confluo.rpc.client import RpcClient client = RpcClient(\"127.0.0.1\", 9090) The first argument to the rpc_client constructor corresponds to the server hostname, while the second argument corresponds to the server port. We then create a new Atomic MultiLog within the Store (synonymous to a database table); as before, this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: ```cpp tab=\"C++\" std::string schema = \"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; client.create_atomic_multilog(\"perf_log\", schema, storage_mode); ```python tab=\"Python\" from confluo.rpc.storage import StorageMode schema = \"\"\"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"\"\" storage_mode = StorageMode.IN_MEMORY client.create_atomic_multilog(\"perf_log\", schema, storage_mode) This operation also internally sets the current Atomic MultiLog for the client to the one we just created (i.e., perf_log ). We can define indexes as follows: ```cpp tab=\"C++\" client.add_index(\"op_latency_ms\"); ```python tab=\"Python\" client.add_index(\"op_latency_ms\") We can also install filters as follows: ```cpp tab=\"C++\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); ```python tab=\"Python\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\") Additionally, we can add aggregates on filters as follows: ```cpp tab=\"C++\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); ```python tab=\"Python\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\") Finally, we can install a trigger on an aggregate as follows: ```cpp tab=\"C++\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\"); ```python tab=\"Python\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\") To load data into the Atomic MultiLog: ```cpp tab=\"C++\" size_t off1 = client.append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = client.append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = client.append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); ```python tab=\"Python\" off1 = client.append([100.0, 0.5, 0.9, \"INFO: Launched 1 tasks\"]) off2 = client.append([500.0, 0.9, 0.05, \"WARN: Server {2} down\"]) off3 = client.append([1001.0, 0.9, 0.03, \"WARN: Server {2, 4, 5} down\"]) Querying data in the Atomic MultiLog is also similar to the Embedded mode API. It is straightforward to retrieve records given their offsets: ```cpp tab=\"C++\" auto record1 = client.read(off1); auto record2 = client.read(off2); auto record3 = client.read(off3); ```python tab=\"Python\" record1 = client.read(off1) record2 = client.read(off2) record3 = client.read(off3) We can query indexed attributes as follows: ```cpp tab=\"C++\" auto record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\") for r in record_stream: print r We can query a pre-defined filter as follows: ```cpp tab=\"C++\" auto record_stream = client.query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" import sys record_stream = client.query_filter(\"low_resources\", 0, sys.maxsize) for r in record_stream: print r We can obtian the value of a pre-defined aggregate as follows: ```cpp tab=\"C++\" std::string value = client.get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value; ```python tab=\"Python\" import sys value = client.get_aggregate(\"max_latency_ms\", 0, sys.maxsize) print value Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: ```cpp tab=\"C++\" auto alert_stream = client.get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); ++s) { std::cout << s.get(); } ```python tab=\"Python\" import sys alert_stream = client.get_alerts(0, sys.maxsize, \"high_latency_trigger\") for a in alert_stream: print a","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"In this Quick Start, we will take a look at how to download and setup Confluo, load some sample data, and query it.","title":"Quick Start"},{"location":"quick_start/#pre-requisites","text":"MacOS X or Unix-based OS; Windows is not yet supported. C++ compiler that supports C++11 standard (e.g., GCC 5.3 or later) CMake 3.2 or later Boost 1.58 or later For python client, you will additionally require: Python 2.7 or later Python Packages: setuptools, six 1.7.2 or later For java client, you will additionally require: Java JDK 1.7 or later ant 1.6.2 or later","title":"Pre-requisites"},{"location":"quick_start/#download-and-install","text":"To download and install Confluo, use the following commands: git clone https://github.com/ucbrise/confluo.git cd confluo mkdir build cd build cmake .. make -j && make test && make install","title":"Download and Install"},{"location":"quick_start/#using-confluo","text":"Confluo can be used in two modes -- embedded and stand-alone. In the embedded mode, Confluo is used as a header-only library in C++, allowing Confluo to use the same address-space as the application process. In the stand-alone mode, Confluo runs as a daemon server process, allowing clients to communicate with it using Apache Thrift protocol.","title":"Using Confluo"},{"location":"quick_start/#embedded-mode","text":"In order to use Confluo in the embedded mode, we simply need to include Confluo's header files under libconfluo/confluo, use the Confluo C++ API in a C++ application, and compile using a modern C++ compiler. The entry point header file to include is confluo_store.h . We will first create a new Confluo Store with the data path for it to use as follows: confluo::confluo_store store(\"/path/to/data\"); We then create a new Atomic MultiLog within the Store (synonymous to a database table); this requires three parameters: name, schema, and the storage mode: std::string schema = \"{ timestamp: LONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; store.create_atomic_multilog(\"perf_log\", schema, storage_mode); Our schema contains 5 attributes: a signed 8-byte integer timestamp, double floating-point precision operation latency (in ms), CPU utilization and available memory, and a string log message field (upper bounded to 100 characters). We then obtain a reference to our newly created Atomic MultiLog: confluo::atomic_multilog* mlog = store.get_atomic_multilog(\"perf_log\"); We can define indexes on the Atomic MultiLog as follows: mlog->add_index(\"op_latency_ms\"); to add an index on op_latency_ms attribute. We can also install filters as follows: mlog->add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); to explicitly filter out records that indicate low system resources (CPU utilization > 80%, Available Memory < 10%), using a filter named low_resources . Additionally, we can add aggregates on filters as follows: mlog->add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); This adds a new stored aggregate max_latency_ms on the filter low_resources we defined before. In essence, it records the highest operation latency reported in any record that also indicated low available resources. Finally, we can install a trigger on aggregates as follows: mlog->install_trigger(\"high_latency_trigger\", \"max_latency > 1000\"); This installs a trigger high_latency_trigger on the aggregate max_latency_ms , which should generate an alert whenever the condition max_latency_ms > 1000 is satisfied, i.e., whenever the maximum latency for an operation exceeds 1s and the available resources are low. We are now ready to load some data into this multilog: size_t off1 = mlog->append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = mlog->append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = mlog->append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); Note that the append method takes a vector of strings as its input, where the vector corresponds to a single record. The number of entries in the vector must match the number of entries in the schema, with the exception of the timestamp --- if the timestamp is not provided, Confluo will automatically assign one. Also note that the operation returns a unique offset corresponding to each append operation. This forms the \"key\" for records stored in the Atomic MultiLog -- records can be retrieved by specifying their corresponding offsets. Now we take a look at how we can query the data in the Atomic MultiLog. First, it is straightforward to retrieve records given their offsets: auto record1 = mlog->read(off1); auto record2 = mlog->read(off2); auto record3 = mlog->read(off3); Each of record1 , record2 , and record3 are vectors of strings. We can query indexed attributes as follows: auto record_stream1 = mlog->execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream1; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } Note that the operation returns a lazily evaluated stream, which supports functional style operations like map, filter, etc. See stream.h for more details. We can also query the defined filter as follows: auto record_stream2 = mlog->query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream2; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The first parameter corresponds to the name of the filter to be queried, while the second and third parameters correspond to the begining timestamp and end timestamp to consider for records in the filter. We've specified them to capture all possible values of timestamp. Similar to the execute_filter query, this operation also returns a lazily evaluated record stream. We query aggregates as follows: auto value = mlog->get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value.to_string(); The query takes the name of the aggregate as its first parameter, while the second and third parameters correspond to begin and end timestmaps, as before. The query returns a numeric object, which is a wrapper around numeric values. Finally, we can query the generated alerts by triggers we have installed as follows: auto alert_stream = mlog->get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); s = s.tail()) { std::cout << s.head().to_string(); } The query takes and begin and end timestamps as its first and second arguments, and an optional trigger name as its third argument. The query returns a lazy stream over generated alerts for this trigger in the specified time-range. See API docs for C++ and in-depth user guides on Data Storage and Conflo Queries for details on Confluo's supported operations.","title":"Embedded Mode"},{"location":"quick_start/#stand-alone-mode","text":"In the stand-alone mode, Confluo runs as a daemon server, serving client requests using Apache Thrift protocol. To start the server, run: confluod --address=127.0.0.1 --port=9090 Once the server daemon is running, you can query it using the C++, Python or Java client APIs. The client APIs closely resemble the embedded API. We first create a new client connection to the Confluo daemon: ```cpp tab=\"C++\" confluo::rpc::rpc_client client(\"127.0.0.1\", 9090); ```python tab=\"Python\" from confluo.rpc.client import RpcClient client = RpcClient(\"127.0.0.1\", 9090) The first argument to the rpc_client constructor corresponds to the server hostname, while the second argument corresponds to the server port. We then create a new Atomic MultiLog within the Store (synonymous to a database table); as before, this requires three parameters: a name for the Atomic MultiLog, a fixed schema, and a storage mode: ```cpp tab=\"C++\" std::string schema = \"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"; auto storage_mode = confluo::storage::IN_MEMORY; client.create_atomic_multilog(\"perf_log\", schema, storage_mode); ```python tab=\"Python\" from confluo.rpc.storage import StorageMode schema = \"\"\"{ timestamp: ULONG, op_latency_ms: DOUBLE, cpu_util: DOUBLE, mem_avail: DOUBLE, log_msg: STRING(100) }\"\"\" storage_mode = StorageMode.IN_MEMORY client.create_atomic_multilog(\"perf_log\", schema, storage_mode) This operation also internally sets the current Atomic MultiLog for the client to the one we just created (i.e., perf_log ). We can define indexes as follows: ```cpp tab=\"C++\" client.add_index(\"op_latency_ms\"); ```python tab=\"Python\" client.add_index(\"op_latency_ms\") We can also install filters as follows: ```cpp tab=\"C++\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\"); ```python tab=\"Python\" client.add_filter(\"low_resources\", \"cpu_util>0.8 || mem_avail<0.1\") Additionally, we can add aggregates on filters as follows: ```cpp tab=\"C++\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\"); ```python tab=\"Python\" client.add_aggregate(\"max_latency_ms\", \"low_resources\", \"MAX(op_latency_ms)\") Finally, we can install a trigger on an aggregate as follows: ```cpp tab=\"C++\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\"); ```python tab=\"Python\" client.install_trigger(\"high_latency_trigger\", \"max_latency_ms > 1000\") To load data into the Atomic MultiLog: ```cpp tab=\"C++\" size_t off1 = client.append({\"100\", \"0.5\", \"0.9\", \"INFO: Launched 1 tasks\"}); size_t off2 = client.append({\"500\", \"0.9\", \"0.05\", \"WARN: Server {2} down\"}); size_t off3 = client.append({\"1001\", \"0.9\", \"0.03\", \"WARN: Server {2, 4, 5} down\"}); ```python tab=\"Python\" off1 = client.append([100.0, 0.5, 0.9, \"INFO: Launched 1 tasks\"]) off2 = client.append([500.0, 0.9, 0.05, \"WARN: Server {2} down\"]) off3 = client.append([1001.0, 0.9, 0.03, \"WARN: Server {2, 4, 5} down\"]) Querying data in the Atomic MultiLog is also similar to the Embedded mode API. It is straightforward to retrieve records given their offsets: ```cpp tab=\"C++\" auto record1 = client.read(off1); auto record2 = client.read(off2); auto record3 = client.read(off3); ```python tab=\"Python\" record1 = client.read(off1) record2 = client.read(off2) record3 = client.read(off3) We can query indexed attributes as follows: ```cpp tab=\"C++\" auto record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\"); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" record_stream = client.execute_filter(\"cpu_util>0.5 || mem_avail<0.5\") for r in record_stream: print r We can query a pre-defined filter as follows: ```cpp tab=\"C++\" auto record_stream = client.query_filter(\"low_resources\", 0, UINT64_MAX); for (auto s = record_stream; !s.empty(); ++s) { std::cout << s.get().to_string(); } ```python tab=\"Python\" import sys record_stream = client.query_filter(\"low_resources\", 0, sys.maxsize) for r in record_stream: print r We can obtian the value of a pre-defined aggregate as follows: ```cpp tab=\"C++\" std::string value = client.get_aggregate(\"max_latency_ms\", 0, UINT64_MAX); std::cout << value; ```python tab=\"Python\" import sys value = client.get_aggregate(\"max_latency_ms\", 0, sys.maxsize) print value Finally, we can obtain alerts generated by triggers installed on an Atomic MultiLog as follows: ```cpp tab=\"C++\" auto alert_stream = client.get_alerts(0, UINT64_MAX, \"high_latency_trigger\"); for (auto s = alert_stream; !s.empty(); ++s) { std::cout << s.get(); } ```python tab=\"Python\" import sys alert_stream = client.get_alerts(0, sys.maxsize, \"high_latency_trigger\") for a in alert_stream: print a","title":"Stand-alone Mode"},{"location":"research/","text":"Research Papers \u00b6 MIND: In-Network Memory Management for Disaggregated Data Centers Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag Khandelwal, Lin Zhong, Abhishek Bhattacharjee SOSP '21, October 26\u201329, Virtual Event, Germany","title":"Research Papers"},{"location":"research/#research-papers","text":"MIND: In-Network Memory Management for Disaggregated Data Centers Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag Khandelwal, Lin Zhong, Abhishek Bhattacharjee SOSP '21, October 26\u201329, Virtual Event, Germany","title":"Research Papers"},{"location":"timeseries_db/","text":"Timeseries Database \u00b6 We describe extensions to Confluo's interface to capture time-series data and support operations similar to BTrDB on the captured data. Implementation \u00b6 Time-series data comprises of a stream of records, each of which is a (timestamp, value) pair. Confluo maintains an index on both the timestamp and the value attribute on the Atomic MultiLog to support queries on time windows as well as more general diagnostic queries. Confluo also supports efficient aggregate queries on the captured time-series data, but via pre-defined aggregates and ad-hoc query execution. Atomic MultiLog offsets implicitly form versions for the timeseries database \u2014 each offset corresponds to a new version of the database and includes all records before that offset. Confluo also permits users to compute the difference between two versions of the database: since database versions map to Atomic MultiLog offsets, Confluo fetches all records that lie between the offsets corresponding to the two versions. Compared Systems and Experimental Setup \u00b6 We evaluate Confluo against BTrDB , CorfuDB and TimescaleDB on c4.8xlarge instances with 18 CPU cores and 60GB RAM. We used the Open \u03bcPMU Dataset , a real- world trace of voltage, current and phase readings col- lected from LBNL\u2019s power-grid over a 3-month period. We create a separate Atomic MultiLog for each type of reading (voltage, current or phase). We run single server benchmarks with 500 million records to highlight the per-server performance of these systems. Requests are issued as continuous streams with 8K record batches. Results \u00b6 Figure: ConfluoD & ConfluoR measure performance for DURABLE & DURABLE_RELAXED modes respectively. ConfluoD achieves 2-20x higher throughput, 2-10x lower latency for inserts, and 1.5-5x higher throughput, 5-20x lower latency for time-range queries than compared systems. The figure above shows that systems like CorfuDB and TimescaleDB achieve over 10x lower performance than BTrDB and Confluo. We emphasize that this is not a shortcoming: CorfuDB and TimescaleDB support stronger (transactional) semantics than BTrDB and Confluo. Thus, depending on desired semantics, either class of systems may be useful for an underlying application. Confluo, using DURABLE_RELAXED writes, is able to achieve close to 27 million inserts/second due to its cheap versioning and lock-free concurrency control. For time-range queries, almost all systems observe similar throughput since queries are served via in-memory indexes. Insertion and query latency trends are similar to throughput trends across different systems.","title":"Timeseries Database"},{"location":"timeseries_db/#timeseries-database","text":"We describe extensions to Confluo's interface to capture time-series data and support operations similar to BTrDB on the captured data.","title":"Timeseries Database"},{"location":"timeseries_db/#implementation","text":"Time-series data comprises of a stream of records, each of which is a (timestamp, value) pair. Confluo maintains an index on both the timestamp and the value attribute on the Atomic MultiLog to support queries on time windows as well as more general diagnostic queries. Confluo also supports efficient aggregate queries on the captured time-series data, but via pre-defined aggregates and ad-hoc query execution. Atomic MultiLog offsets implicitly form versions for the timeseries database \u2014 each offset corresponds to a new version of the database and includes all records before that offset. Confluo also permits users to compute the difference between two versions of the database: since database versions map to Atomic MultiLog offsets, Confluo fetches all records that lie between the offsets corresponding to the two versions.","title":"Implementation"},{"location":"timeseries_db/#compared-systems-and-experimental-setup","text":"We evaluate Confluo against BTrDB , CorfuDB and TimescaleDB on c4.8xlarge instances with 18 CPU cores and 60GB RAM. We used the Open \u03bcPMU Dataset , a real- world trace of voltage, current and phase readings col- lected from LBNL\u2019s power-grid over a 3-month period. We create a separate Atomic MultiLog for each type of reading (voltage, current or phase). We run single server benchmarks with 500 million records to highlight the per-server performance of these systems. Requests are issued as continuous streams with 8K record batches.","title":"Compared Systems and Experimental Setup"},{"location":"timeseries_db/#results","text":"Figure: ConfluoD & ConfluoR measure performance for DURABLE & DURABLE_RELAXED modes respectively. ConfluoD achieves 2-20x higher throughput, 2-10x lower latency for inserts, and 1.5-5x higher throughput, 5-20x lower latency for time-range queries than compared systems. The figure above shows that systems like CorfuDB and TimescaleDB achieve over 10x lower performance than BTrDB and Confluo. We emphasize that this is not a shortcoming: CorfuDB and TimescaleDB support stronger (transactional) semantics than BTrDB and Confluo. Thus, depending on desired semantics, either class of systems may be useful for an underlying application. Confluo, using DURABLE_RELAXED writes, is able to achieve close to 27 million inserts/second due to its cheap versioning and lock-free concurrency control. For time-range queries, almost all systems observe similar throughput since queries are served via in-memory indexes. Insertion and query latency trends are similar to throughput trends across different systems.","title":"Results"},{"location":"type_system/","text":"Confluo Type System \u00b6 Confluo uses a strictly typed system. While primitive data types like BOOL , CHAR , SHORT , INT , LONG , FLOAT , DOUBLE and STRING are supported by default in Confluo, it is possible to add custom user-defined data types. This requires defining a few operations that would allow operations like applying filters and triggers on attributes of the custom data type. Registering Types \u00b6 To create a new type, we need to define the following properties so that native operations can be supported; these properties are summarised in the type_properties struct: std::string name - A unique name for the type size_t size - The size of underlying representation for fixed sized types. This should be set to zero for dynamically sized types (e.g., see definintion for STRING type). void* min - This is a pointer to the minimum value that the type can hold. See type_properties.h to see examples of min assigned to primitive types. void* max - This is a pointer to the maximum value that the type can hold. See type_properties.h to see examples of max assigned to primitive types. void* one - This is a pointer to the step value with which the type can be incremented. See type_properties.h to see examples of one assigned to primitive types. void* zero - This is a pointer to the zero value for the type. See type_properties.h to see examples of zero assigned to primitive types. bool is_numeric - This indicates whether the type is numeric or not; numeric types typically support most arithmetic operators; see [arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples. relational_ops_t relational_ops - Stores a list of relational operator functions for the given type, so that operations like filter can work. See rel_ops.h for examples of what relational functions can be defined. binary_ops_t binary_ops - Stores a list of binary arithmetic operator functions for the given type, so that operations like filter can accurately be applied to the type. Check arithmetic_ops.h for examples of binary operator functions that can be defined. unary_ops_t unary_ops - Stores a list of unary arithmetic operator functions for the given type, so that operations like filter can work for the given type. Check arithmetic_ops.h for examples of unary function operators that can be defined. key_op_t key_transform_op - Stores the key-transform function. This function is important for looking up attributes of the type in an index; see key_ops.h for example definitions of key_transform. parse_op_t parse_op - Parses data instance from a string representation of this type. See string_ops.h for examples. to_string_op_t to_stirng_op - Converts data instance of the type to its string representation. See string_ops.h for examples. serialize_op_t serialize_op - Serializes the underlying data representation of the type into raw bytes; see serde_ops.h for examples. deserialize_op_t deserialize_op - Reads the raw byte representation of the type and parses it to data; see serde_ops.h for examples. Example declarations of user-defined types can be found at ip_address.h and size_type.h . Once the properties for custom type is defined in the type_properties struct, it needs to be registered with Confluo's type manager via the type_manager::register_type interface. We can register a type as follows: type_properties test_properties(\"test\", sizeof(int), &limits::int_min, &limits::int_max, &limits::int_one, &limits::int_one, &limits::int_zero, false, get_relops(), get_unaryops(), get_binaryops(), get_keyops(), &test_type::parse_test, &test_type::test_to_string, &confluo::serialize<test>, &confluo::deserialize<test>); type_manager::register_type(test_properties); Once registered, a useful symbolic reference to the data type, wrapped in a data_type object, can be obtained via the type_manager::get_type interface. With this object, it is possible to add new columns of this type in any Atomic MultiLog. From here on out, appending records to the Atomic MultiLog, along with operations like filters and triggers, will work out of the box. Building a Schema \u00b6 We can create columns of a custom user-defined type for an Atomic MultiLog as follows: data_type test_type = type_manager::get_type(\"test\"); schema_builder builder; builder.add_column(test_type, \"a\"); std::vector<column_t> s = builder.get_columns(); task_pool MGMT_POOL; atomic_multilog dtable(\"table\", s, \"/tmp\", storage::IN_MEMORY, MGMT_POOL); We use the type_manager::get_type interface to get the data_type object associated with the new user-defined type. This object can then be passed into the add_column interface of the schema_builder along with the name of the column. Finally, the vector of columns can be passed into the Atomic Multilog and thus operations like filters and triggers can be performed on data of the user-defined type. Adding Records \u00b6 First create a packed struct containing all of the member data_types in a row of the schema. Be sure to include the timestamp as one of the columns. An example is as follows: struct rec { int64_t ts; test_type a; }__attribute__((packed)); Then get a pointer to an instance of the struct, which is passed into the append method of the Atomic MultiLog. The following is an exmaple: rec r = {utils::time_utils::cur_ns(), test_type()}; void* data = reinterpret_cast<void*>(&r); dtable.append(r); Here we initialize a record with the current timestamp and an instance of the test type. Then we pass a pointer to that data to the append method of the Atomic MultiLog instance. Performing Filter Operations \u00b6 After adding records containing data of the user-defined type, we can perform filter operations to select specific records. We can do so as follows: for (auto r = dtable.execute_filter(\"a > 4\"); !r.empty(); r = r.tail()) { std::cout << \"A value: \" << r.head().at(1).as<test_type>().get_test() << std::endl; } We assume here that 4 can be parsed as a test_type using the parse_test method. The execute_filter function is then called with an expression that selects records that satisfy the condition. This returns a stream of records which can then be read and processed. See type_manager_test.h for examples of building user-defined types including ip address and size types.","title":"Confluo Type System"},{"location":"type_system/#confluo-type-system","text":"Confluo uses a strictly typed system. While primitive data types like BOOL , CHAR , SHORT , INT , LONG , FLOAT , DOUBLE and STRING are supported by default in Confluo, it is possible to add custom user-defined data types. This requires defining a few operations that would allow operations like applying filters and triggers on attributes of the custom data type.","title":"Confluo Type System"},{"location":"type_system/#registering-types","text":"To create a new type, we need to define the following properties so that native operations can be supported; these properties are summarised in the type_properties struct: std::string name - A unique name for the type size_t size - The size of underlying representation for fixed sized types. This should be set to zero for dynamically sized types (e.g., see definintion for STRING type). void* min - This is a pointer to the minimum value that the type can hold. See type_properties.h to see examples of min assigned to primitive types. void* max - This is a pointer to the maximum value that the type can hold. See type_properties.h to see examples of max assigned to primitive types. void* one - This is a pointer to the step value with which the type can be incremented. See type_properties.h to see examples of one assigned to primitive types. void* zero - This is a pointer to the zero value for the type. See type_properties.h to see examples of zero assigned to primitive types. bool is_numeric - This indicates whether the type is numeric or not; numeric types typically support most arithmetic operators; see [arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples. relational_ops_t relational_ops - Stores a list of relational operator functions for the given type, so that operations like filter can work. See rel_ops.h for examples of what relational functions can be defined. binary_ops_t binary_ops - Stores a list of binary arithmetic operator functions for the given type, so that operations like filter can accurately be applied to the type. Check arithmetic_ops.h for examples of binary operator functions that can be defined. unary_ops_t unary_ops - Stores a list of unary arithmetic operator functions for the given type, so that operations like filter can work for the given type. Check arithmetic_ops.h for examples of unary function operators that can be defined. key_op_t key_transform_op - Stores the key-transform function. This function is important for looking up attributes of the type in an index; see key_ops.h for example definitions of key_transform. parse_op_t parse_op - Parses data instance from a string representation of this type. See string_ops.h for examples. to_string_op_t to_stirng_op - Converts data instance of the type to its string representation. See string_ops.h for examples. serialize_op_t serialize_op - Serializes the underlying data representation of the type into raw bytes; see serde_ops.h for examples. deserialize_op_t deserialize_op - Reads the raw byte representation of the type and parses it to data; see serde_ops.h for examples. Example declarations of user-defined types can be found at ip_address.h and size_type.h . Once the properties for custom type is defined in the type_properties struct, it needs to be registered with Confluo's type manager via the type_manager::register_type interface. We can register a type as follows: type_properties test_properties(\"test\", sizeof(int), &limits::int_min, &limits::int_max, &limits::int_one, &limits::int_one, &limits::int_zero, false, get_relops(), get_unaryops(), get_binaryops(), get_keyops(), &test_type::parse_test, &test_type::test_to_string, &confluo::serialize<test>, &confluo::deserialize<test>); type_manager::register_type(test_properties); Once registered, a useful symbolic reference to the data type, wrapped in a data_type object, can be obtained via the type_manager::get_type interface. With this object, it is possible to add new columns of this type in any Atomic MultiLog. From here on out, appending records to the Atomic MultiLog, along with operations like filters and triggers, will work out of the box.","title":"Registering Types"},{"location":"type_system/#building-a-schema","text":"We can create columns of a custom user-defined type for an Atomic MultiLog as follows: data_type test_type = type_manager::get_type(\"test\"); schema_builder builder; builder.add_column(test_type, \"a\"); std::vector<column_t> s = builder.get_columns(); task_pool MGMT_POOL; atomic_multilog dtable(\"table\", s, \"/tmp\", storage::IN_MEMORY, MGMT_POOL); We use the type_manager::get_type interface to get the data_type object associated with the new user-defined type. This object can then be passed into the add_column interface of the schema_builder along with the name of the column. Finally, the vector of columns can be passed into the Atomic Multilog and thus operations like filters and triggers can be performed on data of the user-defined type.","title":"Building a Schema"},{"location":"type_system/#adding-records","text":"First create a packed struct containing all of the member data_types in a row of the schema. Be sure to include the timestamp as one of the columns. An example is as follows: struct rec { int64_t ts; test_type a; }__attribute__((packed)); Then get a pointer to an instance of the struct, which is passed into the append method of the Atomic MultiLog. The following is an exmaple: rec r = {utils::time_utils::cur_ns(), test_type()}; void* data = reinterpret_cast<void*>(&r); dtable.append(r); Here we initialize a record with the current timestamp and an instance of the test type. Then we pass a pointer to that data to the append method of the Atomic MultiLog instance.","title":"Adding Records"},{"location":"type_system/#performing-filter-operations","text":"After adding records containing data of the user-defined type, we can perform filter operations to select specific records. We can do so as follows: for (auto r = dtable.execute_filter(\"a > 4\"); !r.empty(); r = r.tail()) { std::cout << \"A value: \" << r.head().at(1).as<test_type>().get_test() << std::endl; } We assume here that 4 can be parsed as a test_type using the parse_test method. The execute_filter function is then called with an expression that selects records that satisfy the condition. This returns a stream of records which can then be read and processed. See type_manager_test.h for examples of building user-defined types including ip address and size types.","title":"Performing Filter Operations"}]}